{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorenh/Documents/DSaPP/.aequitas/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (bias.py, line 123)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/lorenh/Documents/DSaPP/.aequitas/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3265\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d8defb95cd1a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from aequitas.bias import Bias\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/lorenh/Documents/DSaPP/.aequitas/lib/python3.6/site-packages/aequitas-0.23.0-py3.6.egg/aequitas/bias.py\"\u001b[0;36m, line \u001b[0;32m123\u001b[0m\n\u001b[0;31m    'attribute_value'].values.tolist()[0]\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.preprocessing import preprocess_input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../examples/data/compas_for_aequitas.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Aequitas: Exploring the COMPAS Dataset\n",
    "\n",
    "__Risk assessment by race__\n",
    "\n",
    "COMPAS produces a risk score that predicts a person's likelihood of commiting a crime in the next two years. The output is a score between 1 and 10 that maps to low, medium or high. For Aequitas, we collapse this to a binary prediction. A score of 0 indicates a prediction of \"low\" risk according to COMPAS, while a 1 indicates \"high\" or \"medium\" risk.\n",
    "\n",
    "This categorization is based on ProPublica's interpretation of Northpointe's practioner guide:\n",
    "\n",
    "    \"According to Northpointe’s practitioners guide, COMPAS “scores in the medium and high range garner more interest from supervision agencies than low scores, as a low score would suggest there is little risk of general recidivism,” so we considered scores any higher than “low” to indicate a risk of recidivism.\"\n",
    "\n",
    "In the bar charts below, we see a large difference in how these scores are distributed by race, with a majority of white and Hispanic people predicted as low risk (score = 0) and a majority of black people predicted high and medium risk (score = 1). We also see that while the majority of people in age categories over 25 are predicted as low risk (score = 0), the majority of people below 25 are predicted as high and medium risk (score = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "\n",
    "Data for this example was preprocessed for compatibility with Aequitas. **The Aequitas tool always requires a `score` column and requires a binary `label_value` column for supervised metrics**, (i.e., False Discovery Rate, False Positive Rate, False Omission Rate, and False Negative Rate).\n",
    "\n",
    "Preprocessing includes but is not limited to checking for mandatory `score` and `label_value` columns as well as at least one column representing attributes specific to the data set. See [documentation](../input_data.html) for more information about input data.\n",
    "\n",
    "Note that while `entity_id` is not necessary for this example, Aequitas recognizes `entity_id` as a reserve column name and will not recognize it as an attribute column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top_cell)\n",
    "<a id='existing_biases'></a>\n",
    "## What biases exist in my model?\n",
    "### Aequitas Group() Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='xtab'></a>\n",
    "### What is the distribution of groups, predicted scores, and labels across my dataset?\n",
    "\n",
    "Aequitas's `Group()` class enables researchers to evaluate biases across all subgroups in their dataset by assembling a confusion matrix of each subgroup, calculating commonly used metrics such as false positive rate and false omission rate, as well as counts by group and group prevelance among the sample population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`get_crosstabs()`** command tabulates a confusion matrix for each subgroup and calculates commonly used metrics such as false positive rate and false omission rate. It also provides counts by group and group prevelances.\n",
    "\n",
    "#### Group Counts Calculated:\n",
    "\n",
    "| Count Type | Column Name |\n",
    "| --- | --- |\n",
    "| False Positive Count | 'fp' |\n",
    "| False Negative Count | 'fn' |\n",
    "| True Negative Count | 'tn' |\n",
    "| True Positive Count | 'tp' |\n",
    "| Predicted Positive Count | 'pp' |\n",
    "| Predicted Negative Count | 'pn' |\n",
    "| Count of Negative Labels in Group | 'group_label_neg' |\n",
    "| Count of Positive Labels in Group | 'group_label_pos' | \n",
    "| Group Size | 'group_size'|\n",
    "| Total Entities | 'total_entities' |\n",
    "\n",
    "#### Absolute Metrics Calcuated:\n",
    "\n",
    "| Metric | Column Name |\n",
    "| --- | --- |\n",
    "| True Positive Rate | 'tpr' |\n",
    "| True Negative Rate | 'tnr' |\n",
    "| False Omission Rate | 'for' |\n",
    "| False Discovery Rate | 'fdr' |\n",
    "| False Positive Rate | 'fpr' |\n",
    "| False Negative Rate | 'fnr' |\n",
    "| Negative Predictive Value | 'npv' |\n",
    "| Precision | 'precision' |\n",
    "| Predicted Positive Ratio$_k$ | 'ppr' |\n",
    "| Predicted Positive Ratio$_g$ | 'pprev' |\n",
    "| Group Prevalence | 'prev' |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The **`get_crosstabs()`** method expects a dataframe with predefined columns `score`, and `label_value` and treats other columns (with a few exceptions) as attributes against which to test for disparities. In this cases we include `race`, `sex` and `age_cat`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Group()\n",
    "xtab, _, score_thresholds_dict = g.get_crosstabs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df, _ = preprocess_input_df(df, required_cols=['score', 'label_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step: Incorporate into Bias() class! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will likely want to drop the binary columns\n",
    "Consider: how to keep detail from original df but calculate significance based on the group ID'd as ref_group in bias df?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top_cell)\n",
    "<a id='xtab_metrics'></a>\n",
    "### What are bias metrics across groups?\n",
    "\n",
    "Once you have run the `Group()` class, you'll have a dataframe of the group counts and group value bias metrics.\n",
    "\n",
    "The `Group()` class has a **`list_absolute_metrics()`** method, which you can use for faster slicing to view just  counts or bias metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_metrics = g.list_absolute_metrics(xtab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View calculated counts across sample population groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab[[col for col in xtab.columns if col not in absolute_metrics]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View calculated absolute metrics for each sample population group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = b.get_disparity_predefined_groups(xtab, ref_groups_dict={'sex': 'Female' , 'race': 'Hispanic', 'age_cat': '25 - 45'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab.loc[xtab.group_size.idxmax(), 'attribute_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodf = b.get_disparity_major_group(xtab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_key_columns = ['model_id', 'score_threshold', 'attribute_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min_idx = bodf.loc[bodf.groupby(default_key_columns)['fpr'].idxmin()]\n",
    "df_min_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min_idx.loc[df_min_idx['attribute_name'] == 'race']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.get_disparity_min_metric(xtab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aequitas",
   "language": "python",
   "name": ".aequitas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
